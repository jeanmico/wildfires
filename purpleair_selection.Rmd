---
title: "PurpleAir Selection"
author: "Jean Costello"
date: "2/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r read}
qc_data = read.csv('~/wildfires/purple_qc_data_4.csv')
```

Number of sensors: `r nrow(qc_data)`

Plot of the sensor start date

Histogram of active days
```{r day_hist}
day_hist <- ggplot(qc_data, aes(x=ndays)) +
  geom_histogram() + 
  xlab("Number of days with data in Nov 2018") + 
  ylab("Sensor count") + 
  theme_minimal()

day_hist
```
```{r correlation}
corr_hist <- ggplot(qc_data, aes(x=ab_correlation)) + 
  geom_histogram() + 
  theme_minimal()


corr_hist

```
Note that if one channel is missing (all NA), we assigned a correlation of 0.

Number of sensors above 90%: `r nrow(filter(qc_data, ab_correlation > .9))`
Number of sensors above 95%: `r nrow(filter(qc_data, ab_correlation > .95))`

Restricting to the sensors that are above .9 correlation:
```{r qc_filter}
qc90 <- qc_data %>% filter(ab_correlation > .9)

```

Check repeats in these sensors
```{r repeats_a}
repeat_a_hist <- ggplot(qc90, aes(x=a_repeats)) +
  geom_histogram() +
  theme_minimal() +
  xlab('Longest repeated value string in the A channel') + 
  ylab('Sensor count')

repeat_a_hist
```
```{r repeats_b}
repeat_b_hist <- ggplot(qc90, aes(x=b_repeats)) +
  geom_histogram() +
  theme_minimal() +
  xlab('Longest repeated value string in the B channel') + 
  ylab('Sensor count')

repeat_b_hist
```

Number of sensors with a repeat length <20: `r nrow(filter(qc_data, a_repeats<20))`

Check the number of corrected values (outlier replaced)
```{r outlier_a_hist}
outliers_a_hist <- ggplot(qc90, aes(x=a_outlier_change)) +
  geom_histogram() + 
  theme_minimal()

outliers_a_hist
```

```{r outlier_b_hist}
outliers_b_hist <- ggplot(qc90, aes(x=b_outlier_change)) +
  geom_histogram() + 
  theme_minimal()

outliers_b_hist
```

Narrow in our analysis to the sensors with a correlation > 90%. 

First, we'll aggregate the data to 1-hour and daily time periods. The daily data is what we'll use for our later analysis, but the hourly data gives us a better look at the data quality.

```{r read_data}
library(readr)

ids_list = unique(qc90$deviceDeploymentID)

#TODO: read in files, use Mazama package to hourly or daily average, print smaller dataframe to speed this up!
# cannot read in saved files and convert to pat object
# see: https://github.com/MazamaScience/AirSensor/issues/260

for (i in 1:length(ids_list)) {
  fpath = '/Volumes/Padlock/purple_mz/'
  fname_data = paste(fpath, ids_list[[i]], '_data.csv', sep='')
  fname_meta = paste(fpath, ids_list[[i]], '_meta.csv', sep='') 
  
  tmp_data = read_csv(fname_data)
  tmp_meta = read_csv(fname_meta)
  
  pat_isPat(tmp_pat)
  
  # create a pat object
  
}
```

```{r filepaths}

# download the PurpleAir data for the good ids
# aggregate
#    by hour
#    by day
# add some additional qc metrics (correlation between temp and humidity)

setArchiveBaseUrl("http://data.mazamascience.com/PurpleAir/v1")
all_pas <- pas_load()
  
qc_data_addl <- data.frame(matrix(ncol=2, nrow=0))

colnames(qc_data_addl) = c('deviceDeploymentID', 'ht_correlation')
fpath_hour = '/Volumes/Padlock/purple18_hourly/'
fpath_day = '/Volumes/Padlock/purple18_daily/'
fpath_stdev = '/Volumes/Padlock/purple18_stdev/'
fname_qc = '/Volumes/Padlock/purple18_addlqc/humid_temp.csv'
```

```{r averaging data}

for (i in 1:length(ids_list)) {
  
  fname = paste(ids_list[[i]], '.csv', sep='')
  fname_hour <- paste(fpath_hour, fname, sep = '')
  fname_day <- paste(fpath_day, fname, sep = '')
  fname_stddev <- paste(fpath_stdev, fname, sep = '')
  
  if (!(file.exists(fname_day))) {
    
    # download data
    tmp <- pat_downloadParseRawData(
      id = ids_list[[i]],
      label = NULL,
      pas = ba_pas,
      startdate = '2018-10-31',
      enddate = '2018-12-01',
      timezone = "America/Los_Angeles",
      baseUrl = "https://api.thingspeak.com/channels/"
    )
  
    cmb <- pat_createPATimeseriesObject(tmp)
    
    humid <- cmb$data$humidity
    temp <- cmb$data$temperature
    
    # check correlation between humidity and temp
    if (sum(complete.cases(humid, temp)) > 0) {
      ht_corr <- cor(humid, temp, use = "complete.obs")
    } else {
      ht_corr = 0
    }
    
    qc_tmp = data.frame('deviceDeploymentID' = ids_list[[i]], 
                        'ht_correlation' = ht_corr)
    qc_data_addl <- rbind(qc_data_addl, qc_tmp)
    
    # aggregate hourly
    cmb_hour <- pat_aggregate(cmb)
  
    # aggregate daily
    cmb_day <- pat_aggregate(cmb, unit='hours', count=24)
    
    # 15 minute stdev

    cmb_std <- pat_aggregate(cmb, 
                  function(x) { sd(x, na.rm = TRUE) }, 
                  unit = 'minutes', 
                  count = 15)
    
    # write output
    write.csv(cmb_hour$data, file=fname_hour, row.names = FALSE)
    write.csv(cmb_day$data, file=fname_day, row.names = FALSE)
    write.csv(cmb_std$data, file=fname_stddev, row.names = FALSE)

  }

}
  
write.csv(qc_data_addl, file=fname_qc, row.names=FALSE)  
  
```

Read in additional qc data and merge with existing qc dataframe
```{r additional_qc}
qc_addl = read.csv(fname_qc)

qc90 <- merge(qc90, qc_addl, by = 'deviceDeploymentID')
```

Histogram of humidity and temperature anticorrelation:
```{r temp_humidity}
temp_humid_hist <- ggplot(qc90, aes(x=ht_correlation)) +
  geom_histogram()

temp_humid_hist
```

Read in time-aggregated data (hourly, daily, 15 minute stdev)
```{r read data 2}
library(readr)

ids_list = unique(qc90$deviceDeploymentID)

df_hour = data.frame(matrix(ncol=19, nrow=0))
df_day = data.frame(matrix(ncol=19, nrow=0))
df_stdev = data.frame(matrix(ncol=19, nrow=0))

agg_cols = c("datetime","pm25_A","pm25_B","temperature","humidity","pressure","pm1_atm_A","pm25_atm_A","pm10_atm_A","pm1_atm_B","pm25_atm_B","pm10_atm_B","uptime","rssi","memory","adc0","bsec_iaq","datetime_A","datetime_B")

for (i in c('hour', 'day', 'stdev')) {
  fpath = eval(as.name(paste('fpath', i, sep='_'))) #nonstandard evaluation to choose name of file and dataframe
  
  outdf = data.frame(matrix(ncol=19, nrow=0))
  colnames(outdf) = agg_cols
  outname = paste('df', i, sep='_')

  for (j in 1:length(ids_list)) {
    fname = paste(fpath, ids_list[[j]], '.csv', sep = '')
    tmpdf = read.csv(fname)
    
    outdf = rbind(outdf, tmpdf)
  }
  
  #more jank here!
  assign(outname, outdf)
  
}


```


Check coverage of sensors with a correlation >90% (how many readings do we have for each day?)

```{r check_coverage}
coverage_hist <- ggplot(df_day, aes(x=datetime)) +
  geom_bar()

coverage_hist
```

Check that temperature and humidity are anticorrelated

How many repeats and outliers are acceptable?


